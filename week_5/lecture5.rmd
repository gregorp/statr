---
title: 'StatR 502 Lecture 5: | Simulation, Box-Cox, Stepwise Model Selection, and the  Anatomy of a Package'
author: "Gregor Thomas"
date: "February 4, 2016"
output:
  ioslides_presentation:
    highlight: kate
    incremental: yes
  beamer_presentation:
    colortheme: dolphin
    fig_height: 3
    fig_width: 4
    highlight: kate
    incremental: yes
    theme: boadilla
  slidy_presentation:
    fig_height: 3.5
    fig_width: 5
    highlights: pygments
    incremental: yes
    theme: cosmo
fontsize: 8pt
---

## Announcements:
 
- We'll talk Final Projects tonight. Project Proposals are due Sunday February 15

- Homework writeups

```{r, echo = FALSE, include=FALSE}
packages = c("ggplot2", "arm", "stringr", "MASS", "faraway",
             "magrittr", "reshape2", "tidyr", "gridExtra", "dplyr")
lapply(packages, library, character.only = T)
theme_set(theme_bw(10))
knitr::opts_chunk$set(cache = TRUE)
```

## Tonight's Menu:

* Quick review
* Simulation for Prediction (briefly)
* Box-Cox Transformations
* Stepwise Regression (and the like)
    * Stepwise
    * Other
    * Multiple comparisons
* Projects and Packages

## Quick Review

Things you should know off the top of your head

- AIC: which direction is better?
- What's the difference between AIC and BIC?
- What is the point of linear transformations of data?
- In logistic regression, what does the linear predictor predict?
- Looking at a residuals plot (residuals vs fitted values), what pattern would make you think "I need to transform or try a Poisson GLM?"

## Quick Review - answers

Things you should know off the top of your head

- AIC: which direction is better? *Lower is better.*
- What's the difference between AIC and BIC? *BIC penalizes extra parameters based on the size of the data*
- What is the point of linear transformations of data? *make interpretation easier*
- In logistic regression, what does the linear predictor predict? *the log of the odds ratio, log(p / (1-p))*
- Looking at a residuals plot (residuals vs fitted values), what pattern would make you think "I need to transform or try a Poisson GLM?" *variance that increases with the value*

# Simulation (briefly)

## A naive approach to simulation...

We could use the coefficient estimates as "truth" and simulate data based only on the residual error

- This ignores the uncertainty in the model fitting, which is encoded in the standard errors for the coefficients.

## A little better

A better procedure would be to draw a simulated coefficient from a normal distribution based on the coefficient estimates and standard errors...

- But this still ignores the **covariance** of coefficient estimates.

## Much better {.smaller}

To take the correlation of coefficients into account, we need to think of them as a multivariate distribution.

The correlation matrix is easily accessible with `vcov(your_model)`. We could use this with, say, the `mvtnorm` package which supports multivariate normal and t distributions, or use the `arm::sim()` function which does the hard work for us.

Note that this is more or less the approach Nate Silver and the 538-blog in the popular election forecasting. Poll results, with margins of error, were compiled for each state, then simulations run to see the probability of outcomes in the aggregate.

# Box-Cox Transformations

## Box Cox in theory

Using some clever math, the Box-Cox estimates the likelihood of your model under transformations of a certain form of $y$.

Essentially, we're testing transforms of the form
$(y^\lambda - 1) / \lambda$  for $\lambda \neq 0$
  $\log(y)$              for $\lambda = 0$
But the - 1 and / lambda parts are linear transformations,
so they don't change the model fit.

## Box Cox in practice {.smaller}

Tree data!

```{r}
boxcox(Volume ~ Height + Girth, data = trees,
       lambda = seq(0.1, 0.5, length = 50))
```

The plot is of likelihood (y-axis) vs lambda (x-axis)
We, of course, want to maximize likelihood, but
it's important to retain interpretability. Thus we prefer
*rational* powers like or 1/2 (square root), 1/3 (cube root) or -1/2 (1 over square root). use a log transform if lambda is close to 0.

---

```{r}
mod.log = lm(Volume ~ log(Height) + log(Girth), data = trees)
mod.log.log = lm(log(Volume) ~ log(Height) + log(Girth), data = trees)
mod = lm((Volume) ~ (Height) + (Girth), data = trees)
mod.crt = lm((Volume)^(1/3) ~ (Height) + (Girth), data = trees)
```

## What do we have {.smaller}

```{r}
library(arm)
display(mod.log.log)
display(mod.crt)
```

--- 

```{r, fig.height=6, fig.width = 7, echo = FALSE}
par(mfrow = c(2, 2))
plot(mod.log, which = 1)
mtext("log", adj = 1)
plot(mod.log.log, which = 1)
mtext("log.log", adj = 1)
plot(mod, which = 1)
mtext("vanilla", adj = 1)
plot(mod.crt, which = 1)
mtext("crt", adj = 1)
```

## More reading on Box-Cox

A couple *stats* stack exchange questions (see the answers by whuber)

- <http://stats.stackexchange.com/a/35717/7515>
- <http://stats.stackexchange.com/q/60431/7515>

The Wikipedia page is pretty good:
<http://en.wikipedia.org/wiki/Power_transform>

The Box-Tidwell test is exactly analagous, but for transformations of predictors. It's implemented in `car::boxTidwell`.

# Automated Model Selection

## Model Selection

Many people want to take the analyst out of analysis as much as possible. This is a good goal, but one we're still a good ways from achieving.

Model selection is **the** major problem in modern statistics.

---

Most of the problem is due to the lack of a perfect metric for model performance. If you're willing to focus solely on predictive accuracy, for example, then you can do pretty well with machine learning algorithms (things like random forests, which you'll see next quarter). Unfortunately, to get an *interpretable* model, especially a **causal** model, rather than black box that makes pretty good predictions, there is no silver bullet, and there are problems with selection and bias if you dredge your data too thoroughly.

All that said, let's look at a couple methods for automatically searching the "model space".

## Step-wise Model Selection

Stepwise model selection (forward selection) starts with a simple model, and adds in predictors one at a time, choosing the best to add based on AIC (or BIC or similar).

Backward selection starts with a complex model and removes terms one at a time. 

These are implemented in `MASS::stepAIC`. Setting `direction="both"` will consider adding or removing terms at each step.

(quick demo)

## More thorough searches

You could try all combinations of variables, or all combinations with certain restrictions. The `leaps` package (not just a "step", haha) offers one implementation, another is in `bestglm`, but beware the computation time. Trying all combinations is not a method that scales well with larger (especially wider) data.

Without constraints, trying every combination of inclusion/exclusion means trying $2^k$ models for $k$ variables. Add interactions and it's more like $2^{k^2 / 2}$ models...

## Multiple comparisons

A comic to illustrate: <http://xkcd.com/882/>.

There are corrections you can make to p-values to account for multiple comparisons. The most common is the *Bonferroni correction*, where you divide your cut-off by the number of comparisons you make (or equivalently multiply your p-value).

For example, normally we use $p<0.05$ as the significance threshold, but if we make 5 comparisons Bonferroni would tell us to use $p < 0.05 / 5 = 0.01$. *This can get out of hand quickly and is generally considered overly conservative.*

There are alternatives, such as the Sidák method (you can look it up if you're interested), but it's not clear if any of them apply all that well to a regression setting, *especially* if you start with data that you think might be relevant, rather than a bunch of random noise mixed in with possibly relevant data.

You protect yourself somewhat from multiple comparisons issues in regression by using appropriate data and applying domain-specific knowledge. That said---you do increase your risk by  making more and more comparisons.

## Hate for stepwise selection

```{r}
fortunes::fortune("stepwise", showMatches = T)
```

## Reasons for the hate

- Stepwise regression doesn't thoroughly search the combinations of parameter---it isn't always good at what it's supposed to do.
- Even with the strict penalty of BIC, overfitting is a risk  
- Your p-values aren't really valid anymore because you're making multiple comparisons rather than testing pre-specified hypotheses.
- Regression coefficients in the final fitted model will be biased **away from 0**.
    - Why is this true?

---

Be especially wary of algorithms that use non-penalized metrics, like F-Tests or Chi-Square Tests to decide which parameters to keep. (AIC or BIC are much better in this case.)

Many of these problems can be helped by preferring shrinkage methods such as Lasso or Ridge Regression. You will cover these next quarter.

## Final Projects

- Build a (small) R package, that helps you to
- conduct an analysis on a data set using some of the methods learned this quarter.

Your data set will be included in the package, and your analysis will be written up as package vignette.

## Anatomy of an R Package

R packages aren't all that special, just a collection of files. Nothing has to be compiled, the typical way of distributing package is just to compress the files together in a `.tar.gz` "tarball". 

## Library vs Package

A *library* is a folder on your computer where you store packages. A *package* is a folder that has a specific structure so R knows how to make its functions and documentation available to you when you load it.

You can see libraries on your search path with `.libPaths()` (also used for adding a new library location).

```{r}
.libPaths()
```

## Inside the package folder

The package folder must have the same name as the package. Inside, three components are absolutely necessary:

- `DESCRIPTION` a text file (with no file extension) that has a very specific format for the package name, version, maintainer, author, and dependencies
- `R/` a folder containing `.R` files defining the functions in the package
- `man/` a folder containing `.Rd` documentation files
- `NAMESPACE` another text file with no file extension that has a list of the functions exported 

---

Other contents of the package are optional/depend on "extra" things your package might do. For your final projects, you'll also need:

- `data/` a folder with `.rdata` files for data sets or other non-function objects available in your pacakge.
- `inst/doc/` a folder for vignettes or other large, external documentation files

## Let's poke around

You can look at installed packages in your library. Some will be installed as binaries, which are compressed and not human-readable, but you can download the source on CRAN.

I like to view packages and code on the (unofficial) CRAN mirror on Github: <https://github.com/cran/>. It's nice because you can search for code inside a package.

---

Common things you'll see looking in packages:

- `src/` a folder for non-R code used by the package (C, FORTRAN)
- `README` text file (no file extension) with a brief intro to the package
- `NEWS` describes changes between versions of the package (a well-maintained NEWS file is a hallmark of a really well-maintained package)

More details (and more options) available in [Hadley's online book on R Packages](http://r-pkgs.had.co.nz/).

## Building packages

In this wonderful age, there are lots of tools that help with package building. `devtools` is the main workhorse, it creates the basic structure, build the package (turn it into a tarball), run tests on it, writes your NAMESPACE file, etc.

`roxygen2` is even more useful. If you comment your functions in the roxygen way which we'll cover in lab on Monday, roxygen2 will automatically convert your comments into R documentation. R documentation is written in a sort of lite version of LaTeX, and it's *very* nice to not do edit it manually.

Roxygen also keeps your NAMESPACE file nice and clean.

## Before Monday's lab...

- Install devtools and roxygen2
- If you're on Windows, install RTools
- Run the minimal package R script, and post problems to the forum.
